{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from random import randint\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Activation\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(array,quanti=8):#normalize and quantize the weight, denormalize in feedfoward\n",
    "    w=np.asarray(array)\n",
    "    absmax=np.amax(np.abs(w))\n",
    "    w = w/absmax\n",
    "    quantScale=1/quanti/2\n",
    "    for i in range(-quanti,quanti):\n",
    "        j=i/quanti+quantScale\n",
    "        w[np.abs(w-j)<=quantScale] = j\n",
    "    return w\n",
    "\n",
    "def ifp_normalize(array,levels=16):#normalize and quantize the weight, denormalize in feedfoward\n",
    "    w=np.asarray(array)\n",
    "    absmax=np.amax(np.abs(w))\n",
    "    w = w/absmax\n",
    "    quantScale=1/levels/2\n",
    "    for i in range(0,levels):\n",
    "        j=i/levels+quantScale\n",
    "        w[np.abs(w-j)<=quantScale] = j\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(60000, 784)\n",
    "x_test = X_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train=ifp_normalize(x_train,16)\n",
    "x_test=ifp_normalize(x_test,16)\n",
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(Y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(Y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class WeightClip(Constraint):\n",
    "    '''Clips the weights incident to each hidden unit to be inside a range\n",
    "    '''\n",
    "    def __init__(self, c=1):\n",
    "        self.c = c\n",
    "\n",
    "    def __call__(self, p):\n",
    "        return K.clip(p, -self.c, self.c)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'name': self.__class__.__name__,\n",
    "                'c': self.c}\n",
    "layers=4\n",
    "def propagation_loss(length=10*10e-4,loss_per_length=0.3):\n",
    "    return 1/10**(length*loss_per_length/10)\n",
    "def splitter_loss(loss=0.2):\n",
    "    return 1/10**(loss/10)\n",
    "def coupling_loss(layers=1):\n",
    "    return 1/10**(layers*0.5/10)\n",
    "def layer_loss(leaf=64,first_layer=False):\n",
    "    s_n = math.log2(leaf) + 3\n",
    "    p_n = s_n + 1\n",
    "    c_n = 2\n",
    "    layer_loss = 1/leaf *1/2 * splitter_loss()**(s_n-1) * propagation_loss()**(p_n-1) *coupling_loss()**c_n\n",
    "    if first_layer != True:\n",
    "        layer_loss *= 255/256 * splitter_loss() * propagation_loss()\n",
    "    return layer_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200704    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65536     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65536     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2560      \n",
      "=================================================================\n",
      "Total params: 334,336\n",
      "Trainable params: 334,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "use_bias = False\n",
    "def create_model():\n",
    "    minPDin=2e-7\n",
    "    wq_lv=7\n",
    "    model = Sequential(name='model1')\n",
    "    retrain= False\n",
    "    global layers\n",
    "    global use_bias\n",
    "    if retrain: \n",
    "        for i in range(layers-1):\n",
    "            model.add(Dense(256, input_dim=784,activation=None, use_bias = use_bias, kernel_constraint = WeightClip(1)))\n",
    "            model.add(ReLU(max_value=1/layer_loss(), negative_slope=0.0, threshold=minPDin/layer_loss()))\n",
    "        model.add(Dense(10, activation='softmax',use_bias = use_bias, kernel_constraint = WeightClip(1)))\n",
    "    #model.add(GaussianNoise(noise_para))\n",
    "    else:\n",
    "        for i in range(layers-1):\n",
    "            model.add(Dense(256,input_dim=784,use_bias = use_bias, activation='relu'))\n",
    "        model.add(Dense(10,use_bias = use_bias,activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=tf.keras.optimizers.Adam(), \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "   \n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.3135 - acc: 0.9096 - val_loss: 0.1409 - val_acc: 0.9562\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.1163 - acc: 0.9656 - val_loss: 0.0898 - val_acc: 0.9709\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0756 - acc: 0.9757 - val_loss: 0.0932 - val_acc: 0.9697\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0560 - acc: 0.9821 - val_loss: 0.0753 - val_acc: 0.9748\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0392 - acc: 0.9876 - val_loss: 0.0792 - val_acc: 0.9761\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0316 - acc: 0.9898 - val_loss: 0.0791 - val_acc: 0.9751\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0239 - acc: 0.9921 - val_loss: 0.0657 - val_acc: 0.9816\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0209 - acc: 0.9933 - val_loss: 0.0690 - val_acc: 0.9801\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0205 - acc: 0.9931 - val_loss: 0.0921 - val_acc: 0.9752\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0178 - acc: 0.9939 - val_loss: 0.0740 - val_acc: 0.9802\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0144 - acc: 0.9952 - val_loss: 0.0985 - val_acc: 0.9769\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0133 - acc: 0.9953 - val_loss: 0.0910 - val_acc: 0.9801\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0126 - acc: 0.9956 - val_loss: 0.0793 - val_acc: 0.9815\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0120 - acc: 0.9958 - val_loss: 0.1059 - val_acc: 0.9760\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0103 - acc: 0.9963 - val_loss: 0.1034 - val_acc: 0.9791\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0953 - val_acc: 0.9775\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0107 - acc: 0.9963 - val_loss: 0.1013 - val_acc: 0.9791\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0866 - val_acc: 0.9812\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0942 - val_acc: 0.9816\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.0054 - acc: 0.9983 - val_loss: 0.1002 - val_acc: 0.9816\n",
      "Test loss: 0.10016344624844725\n",
      "Test accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit(x_train, y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=epochs,\n",
    "#                     verbose=1,\n",
    "#                     validation_data=(x_test, y_test))\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('mnist_DACTL_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 10)\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "for i in range(layers-1):\n",
    "    model2.add(Dense(256,input_dim=784,use_bias = use_bias, activation='relu'))\n",
    "model2.add(Dense(10, use_bias = use_bias,activation='softmax'))\n",
    "\n",
    "model2.load_weights('mnist_DACTL_weights.h5')\n",
    "for i in range(0,layers):\n",
    "    print(model2.layers[i].get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feedforward2(layer,inputs,weights,bias,variation,activate_relu=1,leaf=64,spread_branch=1/256):\n",
    "    # Convert signal from electrical domain to optical domain by TL in former layer\\\n",
    "    \n",
    "    inputs[inputs>1]=1\n",
    "    Splitter_Variation=0.05\n",
    "    tl_variation=0.05\n",
    "     \n",
    "    copy_num = weights.shape[1]\n",
    "    spliter_count = math.log2(leaf) + 3\n",
    "    \n",
    "    if layer!=0:\n",
    "        inputs = inputs * np.random.uniform(1-tl_variation,1+tl_variation,inputs.shape) * coupling_loss()\n",
    "        #print(inputs.shape[0])\n",
    "        # Distribute inputs by splitter trees\n",
    "        random_input = np.repeat(inputs,copy_num,0).reshape(inputs.shape[0],copy_num) \\\n",
    "        * spliter_forest(inputs.shape[0],copy_num,leaf,Splitter_Variation,variation,spread_branch)\n",
    "    else:\n",
    "        random_input = np.repeat(inputs,copy_num,0).reshape(inputs.shape[0],copy_num) * xb_first_layer()\n",
    "    # Add variation to splitters for weight calculation \n",
    "    weights_random = weights * np.random.uniform(1-Splitter_Variation,1+Splitter_Variation,weights.shape)\n",
    "    \n",
    "    # Do the weight calculation \n",
    "    result = random_input * weights_random * propagation_loss() * splitter_loss()\n",
    "    \n",
    "    # Convert signal from optical domain to electrical domain by PD with 1:1 splitter for PD selection\n",
    "    result = result * 0.5 * splitter_loss()* propagation_loss()\\\n",
    "    *np.random.uniform(1-Splitter_Variation,1+Splitter_Variation,result.shape) * coupling_loss() \n",
    "    \n",
    "    # Set lower bound for input of PD\n",
    "    \n",
    "    result[np.abs(result)<2e-7]=0\n",
    "    \n",
    "    # Accumulation\n",
    "    result = np.sum (result,0)\n",
    "  \n",
    "    # If it is not the last layer, implement ReLU and set upper bound for output at 1mW\n",
    "    global use_bias\n",
    "    if use_bias!=False:\n",
    "        for i in range(0,layer+1):\n",
    "            bias =  bias * 255/256 * 1/leaf *1/2 *\\\n",
    "                    splitter_loss() ** spliter_count * propagation_loss() ** spliter_count * coupling_loss() ** 2\n",
    "    #     print('o2 bias:',end='')\n",
    "    #     print(bias)\n",
    "    #     global bias_coe\n",
    "    #     bias *= bias_coe\n",
    "    output = result  + bias\n",
    "    global layers\n",
    "    if layer!=layers-1:\n",
    "        output=relu(output)\n",
    "#     print('o2 output:',end='')\n",
    "    #print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def relu(output):\n",
    "  nextinput = np.zeros((output.shape[0]))\n",
    "  for i in range(nextinput.shape[0]):\n",
    "    nextinput[i] = max(0,output[i])\n",
    "  return nextinput\n",
    "#@jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def network(x_test,y_test,variation):#x_test is 1*784, one 28*28 image\n",
    "    weights=[]\n",
    "    bias=[]\n",
    "    wmax=np.zeros(layers)\n",
    "    global use_bias\n",
    "    for i in range(0,layers):\n",
    "        weights.append(np.array(model2.layers[i].get_weights()[0]))\n",
    "        if use_bias == False:\n",
    "            bias.append(0)\n",
    "        else:\n",
    "            bias.append(np.array(model2.layers[i].get_weights()[1]))\n",
    "        wmax[i]=np.amax(np.abs(weights[i]))\n",
    "    o2 = feedforward2(0,x_test,normalize(weights[0]),bias[0],wmax[0],variation)\n",
    "    o2 = amplifier(o2,1/layer_loss(first_layer=True)*wmax[0],0.05)\n",
    "    o2 = feedforward2(1,o2,normalize(weights[1]),bias[1],wmax[1],variation)\n",
    "    o2 = amplifier(o2,1/layer_loss()*wmax[1],0.05)\n",
    "    o2 = feedforward2(2,o2,normalize(weights[2]),bias[2],wmax[2],variation)\n",
    "    o2 = amplifier(o2,1/layer_loss()*wmax[2],0.05)\n",
    "    o2 = feedforward2(3,o2,normalize(weights[3]),bias[3],wmax[3],variation)\n",
    "    pred_out = np.zeros((1,o2.shape[0]))\n",
    "    pred_out[0] = o2\n",
    "    correct_out = np.zeros((1,y_test.shape[0]))\n",
    "    correct_out[0] = y_test\n",
    "    #print(np.amax(pred_out))\n",
    "    return (np.argmax(pred_out) == np.argmax(correct_out))\n",
    "\n",
    "\n",
    "def spliter_tree(root_value=1,leaf=64,variation=0.05):\n",
    "    height=math.ceil(math.log2(leaf))+1\n",
    "    arr=np.ones(int('1'*height, 2))\n",
    "    for i in range(1,height):\n",
    "        left = np.random.uniform(1-variation,1+variation,2**(i-1))*1/2\n",
    "        right = 1-left\n",
    "        branch = np.concatenate((left,right)).flatten('F')*propagation_loss()*splitter_loss()\n",
    "        arr[2**i-1:2**(i+1)-1]=np.repeat(arr[2**(i-1)-1:2**i-1],2)*branch\n",
    "    return arr[2**(height-1)-1:2**height-1]\n",
    "\n",
    "def spliter_grove(total,spliter_variation,amplifier_variation,leaf=64,spread_branch=1/256):\n",
    "    tl_variation = 0.05\n",
    "    tree_num=math.ceil(total/leaf)\n",
    "    arr=np.zeros((tree_num,leaf))\n",
    "    v=np.random.uniform(1-spliter_variation,1+spliter_variation,tree_num)\n",
    "    right=np.zeros(tree_num)\n",
    "    left=np.zeros(tree_num)\n",
    "    right[0]=v[0]*spread_branch\n",
    "    left[0]=1-right[0]\n",
    "    arr[0]=spliter_tree(left[0]*propagation_loss()*splitter_loss(),leaf,spliter_variation)\n",
    "    for i in range(1,tree_num):\n",
    "        right[i]=amplifier(right[i-1]*splitter_loss()*propagation_loss()*coupling_loss(),\\\n",
    "                           1/(spread_branch*splitter_loss()*propagation_loss()*coupling_loss()),\\\n",
    "                           amplifier_variation)*v[i]*spread_branch\n",
    "        left[i]=1-right[i]\n",
    "        arr[i]=spliter_tree(left[i]*propagation_loss()*splitter_loss(),leaf,spliter_variation)\n",
    "    return arr.flatten()[:total] \n",
    "\n",
    "\n",
    "\n",
    "def spliter_forest(input_num=784,grove_len=256,leaf=64,spliter_variation=0.05,amplifier_variation=0.05,spread_branch=1/256):\n",
    "    arr=np.zeros((input_num,grove_len))\n",
    "    for i in range(input_num):\n",
    "        arr[i]=spliter_grove(grove_len,spliter_variation,amplifier_variation,leaf,spread_branch)\n",
    "    return arr \n",
    "\n",
    "def amplifier(x, multiple, amp_vari=0.05):\n",
    "    delta_in= x * np.random.uniform(-amp_vari,amp_vari)\n",
    "    x_out = x * multiple + multiple * 6.25 * delta_in*10**(1/2)\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xb_first_layer(input_num=784,grove_len=256,spliter_variation=0.05,amplifier_variation=0.05,leaf=64):\n",
    "    tl_variation = 0.05\n",
    "    tree_num=math.ceil(grove_len/leaf)\n",
    "    xb = np.ones((input_num,grove_len))\n",
    "    for i in range(input_num):\n",
    "        for j in range(tree_num):\n",
    "            xb[i][j*leaf:j*leaf+leaf]=spliter_tree()*np.random.uniform(1-tl_variation,1+tl_variation)*coupling_loss()\n",
    "    return xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network(x_test[1],y_test[1],0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference with variation 0.050000 current accuracy at 10: 1.0\n",
      "inference with variation 0.050000 current accuracy at 20: 1.0\n",
      "inference with variation 0.050000 current accuracy at 30: 1.0\n",
      "inference with variation 0.050000 current accuracy at 40: 1.0\n",
      "inference with variation 0.050000 current accuracy at 50: 1.0\n",
      "inference with variation 0.050000 current accuracy at 60: 1.0\n",
      "inference with variation 0.050000 current accuracy at 70: 1.0\n",
      "inference with variation 0.050000 current accuracy at 80: 1.0\n",
      "inference with variation 0.050000 current accuracy at 90: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-f01a141c1f56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-102-f01a141c1f56>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(logname)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#             if True:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                 \u001b[0mcorrect_test\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# to test the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-7ae7a1b1b464>\u001b[0m in \u001b[0;36mnetwork\u001b[1;34m(x_test, y_test, variation)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeedforward2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvariation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mamplifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlayer_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeedforward2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mo2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvariation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mamplifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlayer_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeedforward2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mo2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvariation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-95-cdaf87a2515a>\u001b[0m in \u001b[0;36mfeedforward2\u001b[1;34m(layer, inputs, weights, bias, variation, activate_relu, leaf, spread_branch)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Distribute inputs by splitter trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mrandom_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcopy_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcopy_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;33m*\u001b[0m \u001b[0mspliter_forest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcopy_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleaf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSplitter_Variation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvariation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspread_branch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mrandom_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcopy_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcopy_num\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mxb_first_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-7ae7a1b1b464>\u001b[0m in \u001b[0;36mspliter_forest\u001b[1;34m(input_num, grove_len, leaf, spliter_variation, amplifier_variation, spread_branch)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0marr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrove_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspliter_grove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrove_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspliter_variation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mamplifier_variation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleaf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspread_branch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def main(logname='MNIST_DACTL.txt'):\n",
    "    log=open(logname,'w')\n",
    "    log.write(time.strftime(\"%Y-%m-%d %H:%M:%S\\n\", time.localtime()))\n",
    "    log.close()\n",
    "    result=0\n",
    "    j=5\n",
    "    iteration = 100\n",
    "    for k in range(iteration):\n",
    "        correct_test = 0\n",
    "        for i in range(0,x_test.shape[0]):\n",
    "#             if True:\n",
    "            if (network(x_test[i],y_test[i],0.01*j)):\n",
    "                correct_test += 1\n",
    "            if (i+1)%10==0: # to test the function\n",
    "               print ('inference with variation %f current accuracy at %i:'%(0.01*j,i+1),correct_test/(i+1))\n",
    "        log = open(logname,'a')\n",
    "        log.write('The accuracy of inference %d is %f\\n'%(k,correct_test/10000))\n",
    "        result += correct_test/10000\n",
    "    log=open(logname,'a')\n",
    "    log.write('The average accuracy is %f\\n'%(result/iteration))\n",
    "    log.close()\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
