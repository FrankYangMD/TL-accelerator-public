{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from random import randint\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Activation\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(array,quanti=8):#normalize and quantize the weight, denormalize in feedfoward\n",
    "    w=np.asarray(array)\n",
    "    absmax=np.amax(np.abs(w))\n",
    "    w = w/absmax\n",
    "    quantScale=1/quanti/2\n",
    "    for i in range(-quanti,quanti):\n",
    "        j=i/quanti+quantScale\n",
    "        w[np.abs(w-j)<=quantScale] = j\n",
    "    return w\n",
    "\n",
    "def ifp_normalize(array,levels=16):#normalize and quantize the input, denormalize in feedfoward\n",
    "    w=np.asarray(array)\n",
    "    absmax=np.amax(np.abs(w))\n",
    "    w = w/absmax\n",
    "    quantScale=1/levels/2\n",
    "    for i in range(0,levels):\n",
    "        j=i/levels+quantScale\n",
    "        w[np.abs(w-j)<=quantScale] = j\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(60000, 784)\n",
    "x_test = X_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train=ifp_normalize(x_train,16) * 4\n",
    "x_test=ifp_normalize(x_test,16) * 4\n",
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(Y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(Y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200704    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65536     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65536     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2560      \n",
      "=================================================================\n",
      "Total params: 334,336\n",
      "Trainable params: 334,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class WeightClip(Constraint):\n",
    "    '''Clips the weights incident to each hidden unit to be inside a range\n",
    "    '''\n",
    "    def __init__(self, c=1):\n",
    "        self.c = c\n",
    "\n",
    "    def __call__(self, p):\n",
    "        return K.clip(p, -self.c, self.c)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'name': self.__class__.__name__,\n",
    "                'c': self.c}\n",
    "layers=4\n",
    "def propagation_loss(length=10*10e-4,loss_per_length=0.3):\n",
    "    return 1/10**(length*loss_per_length/10)\n",
    "def splitter_loss(loss=0.2):\n",
    "    return 1/10**(loss/10)\n",
    "def coupling_loss(layers=1):\n",
    "    return 1/10**(layers*0.5/10)\n",
    "def layer_loss(leaf = 256):\n",
    "    s_n = math.log2(leaf)+3\n",
    "    p_n = s_n+1\n",
    "    c_n = 2\n",
    "    return 1/leaf *1/2 * (splitter_loss())**(s_n-1) * propagation_loss()**(p_n-1) *coupling_loss()**c_n\n",
    "use_bias = False\n",
    "def create_model():\n",
    "    minPDin=2e-7\n",
    "    wq_lv=7\n",
    "    model = Sequential(name='model1')\n",
    "    retrain= False\n",
    "    global layers\n",
    "    global use_bias\n",
    "    if retrain: \n",
    "        for i in range(layers-1):\n",
    "            model.add(Dense(256, input_dim=784,activation=None, use_bias = use_bias, kernel_constraint = WeightClip(1)))\n",
    "            model.add(ReLU(max_value=1/layer_loss(), negative_slope=0.0, threshold=minPDin/layer_loss()))\n",
    "        model.add(Dense(10, activation='softmax',use_bias = use_bias, kernel_constraint = WeightClip(1)))\n",
    "    #model.add(GaussianNoise(noise_para))\n",
    "    else:\n",
    "        for i in range(layers-1):\n",
    "            model.add(Dense(256,input_dim=784,use_bias = use_bias, activation='relu'))\n",
    "        model.add(Dense(10,use_bias = use_bias,activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=tf.keras.optimizers.Adam(), \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "   \n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.3209 - acc: 0.9059 - val_loss: 0.1470 - val_acc: 0.9565\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.1148 - acc: 0.9652 - val_loss: 0.1136 - val_acc: 0.9645\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: 0.0750 - acc: 0.9769 - val_loss: 0.0860 - val_acc: 0.9718\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0531 - acc: 0.9833 - val_loss: 0.0795 - val_acc: 0.9742\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: 0.0393 - acc: 0.9874 - val_loss: 0.0728 - val_acc: 0.9771\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0332 - acc: 0.9894 - val_loss: 0.0691 - val_acc: 0.9783\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0271 - acc: 0.9910 - val_loss: 0.0821 - val_acc: 0.9755\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0260 - acc: 0.9913 - val_loss: 0.0888 - val_acc: 0.9760\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: 0.0178 - acc: 0.9944 - val_loss: 0.0718 - val_acc: 0.9790\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: 0.0183 - acc: 0.9937 - val_loss: 0.0790 - val_acc: 0.9794\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0152 - acc: 0.9948 - val_loss: 0.0810 - val_acc: 0.9788\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: 0.0154 - acc: 0.9945 - val_loss: 0.0819 - val_acc: 0.9793\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0124 - acc: 0.9955 - val_loss: 0.0895 - val_acc: 0.9790\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: 0.0176 - acc: 0.9940 - val_loss: 0.0836 - val_acc: 0.9794\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0106 - acc: 0.9967 - val_loss: 0.0908 - val_acc: 0.9787\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 19us/sample - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0810 - val_acc: 0.9785\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.0111 - acc: 0.9960 - val_loss: 0.0960 - val_acc: 0.9784\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: 0.0122 - acc: 0.9959 - val_loss: 0.1125 - val_acc: 0.9759\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: 0.0110 - acc: 0.9966 - val_loss: 0.0962 - val_acc: 0.9791\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 18us/sample - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0906 - val_acc: 0.9819\n",
      "Test loss: 0.09057087836968775\n",
      "Test accuracy: 0.9819\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit(x_train, y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=epochs,\n",
    "#                     verbose=1,\n",
    "#                     validation_data=(x_test, y_test))\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31361568\n",
      "0.4523125\n",
      "0.50465083\n",
      "0.32831943\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "for i in range(layers-1):\n",
    "    model2.add(Dense(256,input_dim=784,use_bias = use_bias, activation='relu'))\n",
    "model2.add(Dense(10, use_bias = use_bias,activation='softmax'))\n",
    "\n",
    "model2.load_weights('mnist_DACTL_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feedforward2(layer,inputs,weights,bias,variation,activate_relu=1,leaf=256,spread_branch=1/256):\n",
    "    # Convert signal from electrical domain to optical domain by TL in former layer\\\n",
    "    inputs[inputs>4]=4\n",
    "    Splitter_Variation=0.05\n",
    "    tl_variation=0.05\n",
    "    inputs = inputs * np.random.uniform(1-tl_variation,1+tl_variation,inputs.shape) * coupling_loss() \n",
    "    copy_num = weights.shape[1]\n",
    "    spliter_count = math.log2(leaf) + 3\n",
    "    #print(inputs.shape[0])\n",
    "    # Distribute inputs by splitter trees\n",
    "    random_input = np.repeat(inputs,copy_num,0).reshape(inputs.shape[0],copy_num) \\\n",
    "    * spliter_forest(inputs.shape[0],copy_num)\n",
    "    # Add variation to splitters for weight calculation \n",
    "    weights_random = weights * np.random.uniform(1-Splitter_Variation,1+Splitter_Variation,weights.shape)\n",
    "    \n",
    "    # Do the weight calculation \n",
    "    result = random_input * weights_random * propagation_loss() * splitter_loss()\n",
    "    \n",
    "    # Convert signal from optical domain to electrical domain by PD with 1:1 splitter for PD selection\n",
    "    result = result * 0.5 * splitter_loss()* propagation_loss()\\\n",
    "    *np.random.uniform(1-Splitter_Variation,1+Splitter_Variation,result.shape) * coupling_loss() \n",
    "#     if activate_relu!=1:\n",
    "#         print(np.amax(result))\n",
    "    # Set lower bound for input of PD\n",
    "    \n",
    "    result[np.abs(result)<2e-7]=0\n",
    "    \n",
    "    # Accumulation\n",
    "    result = np.sum (result,0)\n",
    "    # Amplify the result with corresponding coeffient to obtaint correct output of this layer\n",
    "  \n",
    "    # If it is not the last layer, implement ReLU and set upper bound for output at 1mW\n",
    "    global use_bias\n",
    "    if use_bias!=False:\n",
    "        for i in range(0,layer+1):\n",
    "            bias =  bias * 255/256 * 1/leaf *1/2 *\\\n",
    "                    splitter_loss() ** spliter_count * propagation_loss() ** spliter_count * coupling_loss() ** 2\n",
    "    #     print('o2 bias:',end='')\n",
    "    #     print(bias)\n",
    "    #     global bias_coe\n",
    "    #     bias *= bias_coe\n",
    "    output = result  + bias\n",
    "    global layers\n",
    "    if layer!=layers-1:\n",
    "        output=relu(output)\n",
    "#     print('o2 output:',end='')\n",
    "    #print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def relu(output):\n",
    "  nextinput = np.zeros((output.shape[0]))\n",
    "  for i in range(nextinput.shape[0]):\n",
    "    nextinput[i] = max(0,output[i])\n",
    "  return nextinput\n",
    "#@jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def network(x_test,y_test,variation):#x_test is 1*784, one 28*28 image\n",
    "    weights=[]\n",
    "    bias=[]\n",
    "    wmax=np.zeros(layers)\n",
    "    global use_bias\n",
    "    for i in range(0,layers):\n",
    "        weights.append(np.array(model2.layers[i].get_weights()[0]))\n",
    "        if use_bias == False:\n",
    "            bias.append(0)\n",
    "        else:\n",
    "            bias.append(np.array(model2.layers[i].get_weights()[1]))\n",
    "        wmax[i]=np.amax(np.abs(weights[i]))\n",
    "    o2 = feedforward2(0,x_test,normalize(weights[0]),bias[0],wmax[0],variation)\n",
    "#     o2 = amplifier(o2,1/layer_loss()*wmax[0],0.05)\n",
    "    o2 = feedforward2(1,o2,normalize(weights[1]),bias[1],wmax[1],variation)\n",
    "    o2 = amplifier(o2,1/layer_loss()**2,0.05)\n",
    "    o2 = feedforward2(2,o2,normalize(weights[2]),bias[2],wmax[2],variation)\n",
    "#     o2 = amplifier(o2,1/layer_loss()*wmax[2],0.05)\n",
    "    o2 = feedforward2(3,o2,normalize(weights[3]),bias[3],wmax[3],variation)\n",
    "    pred_out = np.zeros((1,o2.shape[0]))\n",
    "    pred_out[0] = o2\n",
    "    correct_out = np.zeros((1,y_test.shape[0]))\n",
    "    correct_out[0] = y_test\n",
    "    #print(np.amax(pred_out))\n",
    "    return (np.argmax(pred_out) == np.argmax(correct_out))\n",
    "\n",
    "\n",
    "def spliter_tree(root_value=1,leaf=256,variation=0.05):\n",
    "    height=math.ceil(math.log2(leaf))+1\n",
    "    arr=np.ones(int('1'*height, 2))\n",
    "    for i in range(1,height):\n",
    "        left = np.random.uniform(1-variation,1+variation,2**(i-1))*1/2\n",
    "        right = 1-left\n",
    "        branch = np.concatenate((left,right)).flatten('F')*propagation_loss()*splitter_loss()\n",
    "        arr[2**i-1:2**(i+1)-1]=np.repeat(arr[2**(i-1)-1:2**i-1],2)*branch\n",
    "    return arr[2**(height-1)-1:2**height-1]\n",
    "\n",
    "\n",
    "def spliter_forest(input_num=784,grove_len=256,leaf=256,spliter_variation=0.05,amplifier_variation=0.05,spread_branch=1/256):\n",
    "    arr=np.zeros((input_num,grove_len))\n",
    "    if grove_len<256:\n",
    "        leaf=64\n",
    "    for i in range(0,input_num):\n",
    "        arr[i]=spliter_tree(1,leaf)[:grove_len]\n",
    "    return arr \n",
    "\n",
    "def amplifier(x, multiple, amp_vari=0.05):\n",
    "    delta_in= x * np.random.uniform(-amp_vari,amp_vari)\n",
    "    x_out = x * multiple + multiple * 6.25 * delta_in*10**(1/2)\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference with variation 0.050000 current accuracy at 10: 1.0\n",
      "inference with variation 0.050000 current accuracy at 20: 1.0\n",
      "inference with variation 0.050000 current accuracy at 30: 1.0\n",
      "inference with variation 0.050000 current accuracy at 40: 1.0\n",
      "inference with variation 0.050000 current accuracy at 50: 1.0\n",
      "inference with variation 0.050000 current accuracy at 60: 1.0\n",
      "inference with variation 0.050000 current accuracy at 70: 1.0\n",
      "inference with variation 0.050000 current accuracy at 80: 1.0\n",
      "inference with variation 0.050000 current accuracy at 90: 1.0\n",
      "inference with variation 0.050000 current accuracy at 100: 1.0\n",
      "inference with variation 0.050000 current accuracy at 110: 1.0\n",
      "inference with variation 0.050000 current accuracy at 120: 0.9833333333333333\n",
      "inference with variation 0.050000 current accuracy at 130: 0.9846153846153847\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-dbe815b2fba9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-dbe815b2fba9>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(logname)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#             if True:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                 \u001b[0mcorrect_test\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# to test the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-68e718f89c0a>\u001b[0m in \u001b[0;36mnetwork\u001b[1;34m(x_test, y_test, variation)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mbias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mwmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeedforward2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvariation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m#     o2 = amplifier(o2,1/layer_loss()*wmax[0],0.05)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mo2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeedforward2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mo2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvariation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-82ff55a7cdb0>\u001b[0m in \u001b[0;36mfeedforward2\u001b[1;34m(layer, inputs, weights, bias, variation, activate_relu, leaf, spread_branch)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Convert signal from optical domain to electrical domain by PD with 1:1 splitter for PD selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msplitter_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mpropagation_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mSplitter_Variation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mSplitter_Variation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcoupling_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m#     if activate_relu!=1:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#         print(np.amax(result))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def main(logname='MNIST_10mWTL_NoAmp.txt'):\n",
    "    log=open(logname,'w')\n",
    "    log.write(time.strftime(\"%Y-%m-%d %H:%M:%S\\n\", time.localtime()))\n",
    "    log.close()\n",
    "    result=0\n",
    "    j=5\n",
    "    iteration = 100\n",
    "    for k in range(iteration):\n",
    "        correct_test = 0\n",
    "        for i in range(0,x_test.shape[0]):\n",
    "#             if True:\n",
    "            if (network(x_test[i],y_test[i],0.01*j)):\n",
    "                correct_test += 1\n",
    "            if (i+1)%10==0: # to test the function\n",
    "               print ('inference with variation %f current accuracy at %i:'%(0.01*j,i+1),correct_test/(i+1))\n",
    "        log = open(logname,'a')\n",
    "        log.write('The accuracy of inference %d is %f\\n'%(k,correct_test/10000))\n",
    "        result += correct_test/10000\n",
    "    log=open(logname,'a')\n",
    "    log.write('The average accuracy is %f\\n'%(result/iteration))\n",
    "    log.close()\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
